{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Self-Taught Dev","text":"<p>What am I Writing? </p> <p>Explore My Curriculum </p>"},{"location":"#building-software-skills-exploring-computer-science-and-sharing-the-journey","title":"Building software skills, exploring computer science and sharing the journey","text":""},{"location":"#welcome-thank-you-for-visiting","title":"Welcome! Thank you for visiting","text":""},{"location":"#im-tim-mackay-improvising-my-own-path-in-software-development-no-traditional-degreejust-a-passion-for-coding-computer-science-and-self-improvement","title":"I'm Tim MacKay, improvising my own path in software development. No traditional degree\u2014just a passion for coding, computer science and self-improvement.","text":""},{"location":"#as-i-study-i-publish-what-i-learn-these-writings-help-me-learn-and-i-hope-they-can-help-you-too","title":"As I study, I publish what I learn. These writings help me learn and I hope they can help you too!","text":""},{"location":"about/","title":"About Me","text":""},{"location":"about/#tim-mackay","title":"Tim MacKay","text":""},{"location":"about/#notable-experiences","title":"Notable Experiences","text":"<p>2024 \u2014 Successfully created and launched  this website!</p> <p>2023 \u2014 Conference Speaker  at DSTL AI Fest 5 , lauded as a \"confident, clear communicator\" delivering to hundreds of people attending on premise and remotely</p> <p>2022 \u2014 Deployed on UK Specialist Military Operations  to develop novel software</p> <p>2021 \u2014 Presented to 10 Downing Street Data Science team  as an exemplar in code quality and QA</p>"},{"location":"about/#summary","title":"Summary","text":"<p>Highly skilled Full Stack Developer from a military background with over 15 years of experience in technical roles within and supporting UK Defence.</p> <p>Specialised in Software Development since 2018. Dedicated to the craftsmanship of clean and maintainable code. Excellent communication skills  with experience briefing senior executives. Experience working in teams using Scrum. Experience keeping legacy systems operational as well as implementing cutting-edge technologies. Track record of success in improving processes and implementing innovative solutions.</p>"},{"location":"about/#skills-expertise","title":"Skills &amp; Expertise","text":"<ul> <li> <p>Backend</p> <ul> <li>  Python<ul> <li>  FastAPI</li> <li> Pydantic</li> <li> Typer</li> </ul> </li> <li> SQL</li> </ul> </li> <li> <p>Frontend</p> <ul> <li> JavaScript<ul> <li> React</li> <li> TypeScript</li> </ul> </li> <li> CSS</li> <li> HTML</li> </ul> </li> <li> <p>Tools</p> <ul> <li> Git</li> <li> CI Pipelines</li> <li> Docker</li> <li> Gitlab</li> <li> VS Code</li> </ul> </li> <li> <p>Misc</p> <ul> <li> OWASP Top 10</li> <li> Agile</li> <li> Scrum</li> <li> Mac | Linux | Windows</li> </ul> </li> </ul>"},{"location":"about/#work-experience","title":"Work Experience","text":""},{"location":"about/#data-edge-analytics-contractor","title":"Data-Edge Analytics (Contractor)","text":"<p>Jan 2024  Present</p> <ul> <li>Main Tech Stack: Python, Pydantic, Node.js, React, Docker, Gitlab, MKDocs</li> <li>Work with data scientists  to integrate their code into a larger software eco-system</li> <li>Standardise the containerisation and deployment of data science projects within a monorepo</li> <li>Create, maintain and deploy python packages, with hosted documentation, using CI/CD Gitlab pipelines</li> <li>Standardise data transformation between various data processing libraries</li> </ul>"},{"location":"about/#specialist-military-unit-software-and-ai-team-mod-founding-member","title":"Specialist Military Unit Software and AI Team, MoD (Founding Member)","text":"<p>Feb 2022  Oct 2023</p> <ul> <li>Main Tech Stack: Python, FastAPI, Vue, Docker, Azure DevOps</li> <li>Led code QA by advocating and implementing automated code analysis, unit testing, code linters, technical documentation, and other quality assurance measures </li> <li>Developed a robust data pipeline to ingest and visualise telemetry data for enhanced insights and data-driven decision-making</li> <li>Implemented Computer Vision to enhance reconnaissance missions in deployed space</li> <li>Worked on various systems, including Frontends, REST APIs, Databases, Raspberry Pi and Sat Comms </li> </ul>"},{"location":"about/#no1-aeronautical-information-documents-unit-royal-air-force","title":"No1 Aeronautical Information Documents Unit, Royal Air Force","text":"<p>Aug 2018  Feb 2022</p> <ul> <li>Main Tech Stack: PHP, MySQL, JavaScript, jQuery, Foundation, CSS, HTML, Git</li> <li>Mitigated critical security vulnerabilities  in alignment with OWASP Top 10  guidelines, ensuring robust security measures</li> <li>Introduced dependency management and standardised API endpoints</li> <li>Implemented end-to-end automated testing with Cypress </li> <li>Implemented project and knowledge management with Jira and Confluence</li> <li>Trained three junior developers and up-skilled two line managers</li> </ul>"},{"location":"about/#before-2018","title":"Before 2018","text":"<p>In 2009, I embarked on my journey with the Royal Air Force as an Aeronautical Cartographer, immersing myself deeply in the realm of Aeronautical Data . While this document is dedicated to showcasing my career in Software Development, which commenced in earnest in 2018 (see above ), I'd like to provide a brief outline of the skills and experiences acquired in this trade that I believe have significantly enhanced my capabilities as a Developer:</p> <ul> <li>Maintained a database of flight procedures for autopilot systems</li> <li>Implemented over two hundred QA data checks using SQL that led to the improvement of tens of thousands of data records </li> <li>Conducted QA checks on aeronautical charts generated from data</li> <li>Created technical documentation to aid colleagues and improve data quality</li> </ul>"},{"location":"degree/","title":"The Self-Taught Degree","text":"The Journey <p>I've opted for a self-directed learning path over a traditional degree, aiming for a balance of practical software skills, computer science, and vital life skills like leadership and finance. I publish what I learn, providing a resource for both myself and others.</p> <p>My curriculum features both structured and unstructured learning and will evolve with me as I learn. This approach ensures a tailored education aligned with my professional goals.</p> Legend/Key <p>Symbols used in the rest of this page.</p> <ul> <li> : Published Book</li> <li> : Open University Module</li> <li> : YouTube Playlist or Video</li> <li> : Other Online Resources</li> <li> <ul> <li> : Unfinished Module/Resource</li> </ul> </li> <li> <ul> <li> : Finished Module/Resource</li> </ul> </li> </ul>"},{"location":"degree/#stage-1","title":"Stage 1","text":""},{"location":"degree/#introduction-to-computing","title":"Introduction to Computing","text":"<ul> <li> <p> Introduction to computing and information technology 1 (TM111) </p> </li> <li> <p> Introduction to computing and information technology 2 (TM112) </p> </li> <li> <p> CS50's Introduction to Computer Science </p> </li> <li> <p> Think Like a Programmer: An Introduction to Creative Problem Solving </p> </li> <li> <p> Code: The Hidden Language of Computer Hardware and Software </p> </li> </ul>"},{"location":"degree/#programming","title":"Programming","text":"<ul> <li> <p> Composing Programs </p> </li> <li> <p> Structure and Interpretation of Computer Programs: JavaScript Edition </p> </li> </ul>"},{"location":"degree/#software-design","title":"Software Design","text":"<ul> <li> <p> A Philosophy of Software Design </p> </li> <li> <p> The Pragmatic Programmer: Your Journey to Mastery </p> </li> <li> <p> The Software Craftsman: Professionalism, Pragmatism, Pride </p> </li> <li> <p> Modern Software Engineering: Doing What Works to Build Better Software Faster </p> </li> <li> <p> Clean Code: A Handbook of Agile Software Craftsmanship </p> </li> <li> <p> Code Complete: A Practical Handbook of Software Construction </p> </li> </ul>"},{"location":"degree/#software-testing","title":"Software Testing","text":"<ul> <li> <p> Python Testing with pytest: Simple, Rapid, Effective, and Scalable </p> </li> <li> <p> Unit Testing Principles, Practices, and Patterns </p> </li> <li> <p> Test-Driven Development: By Example </p> </li> <li> <p> Agile Testing: A Practical Guide for Testers and Agile Teams </p> </li> </ul>"},{"location":"degree/#mathematics","title":"Mathematics","text":"<ul> <li> <p> Discovering Mathematics (MU123) </p> </li> <li> <p> Essential Mathematics 1 (MST124) </p> </li> <li> <p> Introductory Discrete Mathematics </p> </li> </ul>"},{"location":"degree/#computer-architecture","title":"Computer Architecture","text":"<ul> <li> <p> The Elements of Computing Systems: Building a Modern Computer from First Principles </p> </li> <li> <p> Companion Coursera Course: Building a Modern Computer from First Principles </p> </li> </ul>"},{"location":"degree/#stage-2","title":"Stage 2","text":""},{"location":"degree/#algorithms-data-structures","title":"Algorithms &amp; Data Structures","text":"<ul> <li> <p> Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People </p> </li> <li> <p> A Common-Sense Guide to Data Structures and Algorithms in Python, Volume 1 </p> </li> <li> <p> Algorithms, Data Structures and Computability (M269) </p> </li> <li> <p> Cracking the Coding Interview </p> </li> <li> <p> How to Solve It: A New Aspect of Mathematical Method </p> </li> <li> <p> LeetCode: Top Interview 150 </p> </li> </ul>"},{"location":"degree/#security","title":"Security","text":"<ul> <li> <p> CS50's Introduction to Cybersecurity </p> </li> <li> <p> Web Application Security: Exploitation and Countermeasures for Modern Web Applications </p> </li> </ul>"},{"location":"degree/#object-oriented-programming","title":"Object-Oriented Programming","text":"<ul> <li> <p> Python Object-Oriented Programming </p> </li> <li> <p> The Object-Oriented Thought Process </p> </li> <li> <p> Head First Design Patterns </p> </li> <li> <p> Patterns of Enterprise Application Architecture </p> </li> </ul>"},{"location":"degree/#wrangling-existing-code","title":"Wrangling Existing Code","text":"<ul> <li> <p> Refactoring: Improving the Design of Existing Code </p> </li> <li> <p> Working Effectively with Legacy Code </p> </li> </ul>"},{"location":"degree/#managing-it","title":"Managing IT","text":"<ul> <li> <p> Software Requirements Essentials: Core Practices for Successful Business Analysis </p> </li> <li> <p> User Stories Applied: For Agile Software Development </p> </li> <li> <p> The Mythical Man-Month: Essays on Software Engineering </p> </li> </ul>"},{"location":"degree/#stage-3","title":"Stage 3","text":""},{"location":"degree/#other-ou-modules","title":"Other OU Modules","text":"<ul> <li> <p> Communication and information technologies (TM255) </p> </li> <li> <p> Software engineering (TM354) </p> </li> <li> <p> Communications technology (TM355) </p> </li> </ul>"},{"location":"degree/#operating-systems","title":"Operating Systems","text":"<ul> <li> <p> How Linux Works: What Every Superuser Should Know </p> </li> <li> <p> Operating Systems: Three Easy Pieces </p> </li> </ul>"},{"location":"degree/#computer-networking","title":"Computer Networking","text":"<ul> <li> <p> Lectures (Stanford CS144): Introduction to Computer Networking </p> </li> <li> <p> Computer Networking: A Top-Down Approach </p> </li> </ul>"},{"location":"degree/#databases","title":"Databases","text":"<ul> <li> <p> Lectures (CMU) Intro to Database Systems </p> </li> <li> <p> Architecture of a Database System </p> </li> <li> <p> Readings in Database Systems </p> </li> <li> <p> Data and Reality: A Timeless Perspective on Perceiving and Managing Information in Our Imprecise World </p> </li> </ul>"},{"location":"degree/#languages-and-compilers","title":"Languages and Compilers","text":"<ul> <li> Crafting Interpreters </li> </ul>"},{"location":"degree/#distributed-systems","title":"Distributed Systems","text":"<ul> <li> <p> Lectures (MIT 6.824): Distributed Systems </p> </li> <li> <p> Understanding Distributed Systems </p> </li> <li> <p> Designing Data-Intensive Applications </p> </li> </ul>"},{"location":"degree/#stage-4-a-stage-unstructured","title":"Stage 4 \u2014 A Stage Unstructured","text":"<p>Every resource in this stage is independent and can be completed at any time.</p>"},{"location":"degree/#study-skills","title":"Study Skills","text":"<ul> <li> <p> A Mind for Numbers: How to Excel at Math and Science </p> </li> <li> <p> How to Improve your Critical Thinking &amp; Reflective Skills </p> </li> <li> <p> How to Write Essays &amp; Assignments </p> </li> </ul>"},{"location":"degree/#personal-development","title":"Personal Development","text":"<ul> <li> <p> Atomic Habits </p> </li> <li> <p> As a Man Thinketh </p> </li> <li> <p> Maximum Achievement: Strategies and Skills That Will Unlock Your Hidden Powers to Succeed </p> </li> <li> <p> How to Fail at Almost Everything and Still Win Big </p> </li> <li> <p> The Pathless Path: Imagining a New Story For Work and Life </p> </li> <li> <p> How to Win Friends and Influence People </p> </li> </ul>"},{"location":"degree/#motivation","title":"Motivation","text":"<ul> <li> <p> The Obstacle is the Way: The Ancient Art of Turning Adversity to Advantage </p> </li> <li> <p> The 10X Rule: The Only Difference Between Success and Failure </p> </li> <li> <p> The War of Art </p> </li> <li> <p> Bounce: The of Myth of Talent and the Power of Practice </p> </li> <li> <p> The Chimp Paradox </p> </li> </ul>"},{"location":"degree/#agile","title":"Agile","text":"<ul> <li> <p> Scrum: The Art of Doing Twice the Work in Half the Time </p> </li> <li> <p> Clean Agile: Back to Basics </p> </li> <li> <p> Extreme Programming Explained: Embrace Change </p> </li> <li> <p> Wild West to Agile: Adventures in Software Development Evolution and Revolution </p> </li> <li> <p> Accelerate: The Science of Lean Software and Devops </p> </li> </ul>"},{"location":"degree/#leadership","title":"Leadership","text":"<ul> <li> <p> High Output Management </p> </li> <li> <p> Turn The Ship Around!: A True Story of Turning Followers Into Leaders </p> </li> <li> <p> Start With Why </p> </li> <li> <p> Leaders Eat Last: Why Some Teams Pull Together and Others Don't </p> </li> </ul>"},{"location":"degree/#finance","title":"Finance","text":"<ul> <li> <p> Money: Know More, Make More, Give More </p> </li> <li> <p> Money: A User\u2019s Guide </p> </li> </ul>"},{"location":"degree/#business","title":"Business","text":"<ul> <li> <p> GCSE Business </p> </li> <li> <p> The E-Myth Contractor </p> </li> <li> <p> The E-Myth Revisited </p> </li> <li> <p> Million Dollar Consulting </p> </li> <li> <p> Sell or Be Sold: How to Get Your Way in Business and in Life </p> </li> <li> <p> Rich Dad Poor Dad </p> </li> </ul>"},{"location":"degree/#misc","title":"Misc","text":"<ul> <li> <p> Docker Deep Dive </p> </li> <li> <p> Lectures (MIT) The Missing Semester </p> </li> <li> <p> Soft Skills: The Software Developer's Life Manual </p> </li> <li> <p> Pro Git </p> </li> <li> <p> Microsoft Azure AZ-900 </p> </li> <li> <p> Microsoft Azure AI-900 </p> </li> <li> <p> Touch Typing</p> </li> <li> <p> Give Vim a chance</p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/chunk-it-up/","title":"Chunk It Up!","text":"<p>In the fast-paced world of learning, mastering new information can feel overwhelming. Whether you're tackling a new programming language, absorbing complex technical concepts, or even learning a new skill, our brains are constantly bombarded with data. To manage this flood of information, one of the most effective and natural strategies is chunking.</p>","tags":["education","study","personal-development"]},{"location":"blog/2024/chunk-it-up/#what-is-a-chunk","title":"What is a Chunk?","text":"<p>Chunking is an amazing and surprisingly natural way to learn new information. The basic idea is to isolate a piece of information as a single unit. This could be a concept or a fact etc. Sometimes these chunks may forever stay a single unit, but often as you continue to learn they compound into new chunks and link to other chunks. Let me demonstrate using a concept with which most of us should be familiar.</p> <p>As children we are initially taught to recognise single letters. We go over them again and again to ensure we recognise them visually and audibly. At this early stage, each letter is a Chunk of information. Eventually we memorise the letters and our parents and teachers can point to a letter at random and we immediately vocalise its phonic. Then we start to string those letters together into words.</p> <p>To begin with it's a lengthy task of processing each letter one by one before we know what the word is. But soon that particular combination of letters becomes one whole word. At this point the word is now its own Chunk. A composite of smaller Chunks if you will. At this point we only look at the word and the meaning of all the letters combined enters into our mind.</p>","tags":["education","study","personal-development"]},{"location":"blog/2024/chunk-it-up/#why-is-this-relevant","title":"Why is this Relevant?","text":"<p>Working memory is the part of our brain that is used to reason and make decisions. But it can only juggle so much information at once. Have you ever had so many things happening that you felt unable to focus at all? This is your working memory being overloaded.</p> <p>When we first start creating a chunk of information, it might take up our entire working memory. But as we grapple with the idea, connect it with other knowledge and generally begin to understand it, it takes up less space. It compresses, if you will. The more we understand the chunk the smaller it becomes and the easier it is to hold it in our working memory alongside other chunks.</p> <p>Going back to the reading example, we progress from reading letters, to reading words, to attaching meaning to those words and eventually we can read a book and visualise the entire scene being described. For a child, reading can be exhausting. But as teenagers and adults we have compressed the knowledge to the point where reading can be relaxing.</p> <p>This is the end goal: for the information we are studying to become second nature. Your brain is going to chunk the information whether you consciously help it or not. Might as well lend a hand, right?</p>","tags":["education","study","personal-development"]},{"location":"blog/2024/chunk-it-up/#the-best-way-to-create-a-chunk","title":"The Best Way to Create a Chunk","text":"","tags":["education","study","personal-development"]},{"location":"blog/2024/chunk-it-up/#focus","title":"Focus","text":"<p>Engage Focus Mode! You start by focusing intentionally on the information you are learning. Actively minimise distractions.</p>","tags":["education","study","personal-development"]},{"location":"blog/2024/chunk-it-up/#understand","title":"Understand","text":"<p>Take the time to ensure that you understand the information. Split your time between Focus and Diffuse modes to help your brain process the new information, especially if you find the topic difficult. Follow along with any example problems and ensure you understand the purpose of each step in the solution. Test yourself to validate your understanding. Many educational resources provide additional problem sets to reinforce learning.</p> <p>Tip</p> <p>LLMs such as ChatGPT are excellent resources that can generate additional questions that you can use to test yourself. But make sure you instruct the LLM to double check its own answers so you aren't led off course.</p>","tags":["education","study","personal-development"]},{"location":"blog/2024/chunk-it-up/#contextualise","title":"Contextualise","text":"<p>Knowledge needs to be applied in order to be useful. Once you understand the chunk, make sure you know when to use it. If your resource doesn't adequately explain this then you may have to do your own research. I have personally used ChatGPT many times to shine a light on when something would need to be used. There is little purpose having a screwdriver if you can't recognise an opportunity to use it!</p> <p>This step will help you see how the chunk fits into its surroundings. It will give you a peak at the bigger picture. And each time you recognise when one chunk connects to another via the big picture, you reinforce both chunks! Two for the price of one.</p>","tags":["education","study","personal-development"]},{"location":"blog/2024/chunk-it-up/#the-best-way-to-remember-a-chunk","title":"The Best Way to Remember a Chunk","text":"<p>A common habit for learners is to re-read material as a form of studying. And when you gloss over the material \u2014 or your own notes \u2014 and recognise the information it can lead to a false sense of knowledge. </p> <p>Warning</p> <p>Recognising is not the same as remembering!</p> <p>Instead we need to use recall. Which means pulling the information out of your own memory. At its most basic, recall is as simple as covering the material you're reading and trying to recall what you just read. What was the main idea? What was the chunk?</p> <p>Train yourself to identify main ideas. It's tempting to feel you have to remember everything you read. In truth, a lot of the content is there to emphasise the main ideas. Identify those main ideas and the learning burden lessens.</p> <p>Taking the practice further requires testing yourself routinely. Flashcards are the traditional technique here. Physical or digital. If you can create the questions yourself then even better! Try to at least test yourself on a subject before you go to sleep and the day after you studied it. The retrieval process helps to ingrain these chunks into our long-term memory. An effective way to implement recall is Spaced Repetition.</p> <p>This post is part of a series: How We Learn</p> <ol> <li>The Two Learning Modes</li> <li>Chunk It Up</li> <li>Spaced Repetition</li> <li>How To Remember Anything</li> </ol> <p>This content was inspired by A Mind for Numbers: How to Excel at Math and Science \u2014 By Barbara Oakley, Ph.D.</p> Share this article with your friends? <p> </p>","tags":["education","study","personal-development"]},{"location":"blog/2024/how-to-remember-anything/","title":"How To Remember Anything","text":"<p>Unless you have a particularly gifted memory, most people would say their memory is lacking at best. But a lot of what modern day people are trying to remember is surprisingly abstract. A person's name is just a sound that we associate with them. You can't see or feel a date: it's a globally agreed upon framework to structure time that tries its best to fit in with the earth's orbit around the sun. Our brains struggle with abstract.</p> <p>We don't struggle quite so much with visual and spatial memory. You can see a face you haven't seen in years and know that you've met that person before \u2014 remembering their name is another question. After a couple of commutes to a new place of work you stop  consciously thinking about how to get there. You can probably remember the layout of a houses you grew up in, or a friends house you used to visit. Can you still remember certain routes from one classroom to another in a school you attended?</p> <p>All of this means that we can hijack our brain's ability to remember by choosing how we form the memories. Using our senses to imagine how something might look, smell, sound, feel and taste massively increases the chance that we remember something. Let's go over some techniques.</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/how-to-remember-anything/#memory-palace","title":"Memory Palace","text":"<p>A memory palace starts with any location you can call to memory. It has to be familiar enough that you can imagine walking around it. Some examples could be your house, a place of work, your route to the shops etc. As long as your memory of the location is strong enough you could even use fictional locations from your favourite video game!</p> <p>Then you \"fill\" your memory palace with the stuff you want to remember, all the while using creative imagery to make the information more memorable. Let's use a small shopping list to step through an example. The things you need to buy are: bread, butter, milk and eggs.</p> <p>Imagine walking up to your front door. You pull the door open only to find yourself facing a wall of bread loaves. The loaves are stacked up as if they were bricks. There is a strong smell of freshly baked bread. You notice that instead of cement, these \"brick\" loaves are stuck together using large amounts of butter. Some of the loaves have butter dripping down them where too much has been used. Wanting to get into your house, you push through the wall of bread like a scene from an action movie and arrive in your living room.</p> <p>Sitting on your couch is a huge black and white cow! (This represents the milk.) It's even sitting like a human would. What sounds might it be making? Can you smell the fresh cow pats? Your presence frightens the cow and it begins throwing eggs at you in defiance. You can feel the eggs smashing and cracking against your body and even feel the egg white dripping down your face.</p> <p>I hope you get the idea. Identify anchor points in your memory palace to attach creative images to. In this example I only used two anchor points: the front door and the couch. As you need to remember more things you just increase the journey around your memory palace, adding items to specific anchor points. Some people get really creative with anchor points. I have read about people using their wallets and using each corner of each bank card used as an anchor for different information!</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/how-to-remember-anything/#memorable-sentences","title":"Memorable Sentences","text":"<p>In school, to learn the clockwise order of the compass points (North, South, East &amp; West) we were taught the sentence \"Never Eat Shredded Wheat\", where the first letter of each word maps to the first letter of a compass point.</p> <p>In the same fashion, learning the strings of a guitar (EADGBE), I was taught the morbid sentence \"Eddie Ate Dynamite, Good-Bye Eddie\". Conversely, my wife grew up with the sentence \"Every Angry Dad Gets Big Ears\".</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/how-to-remember-anything/#songs","title":"Songs","text":"<p>Using the memorable medium of song, we can string together the information we need to learn to make it stick in our minds. Granted, a lot of the content out there is aimed more at younger audiences (such as MC Grammar ), but that doesn't stop channels like AcapellaScience  from tackling harder subjects. Want to learn coding concepts such as variables and data types through rap ?</p> <p>If you are particularly musical, this technique could see you becoming the person who creates the songs.</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/how-to-remember-anything/#metaphors-and-analogies","title":"Metaphors and Analogies","text":"<p>Have you ever heard it said that Italy looks like a boot? Or what about electrical voltage being compared to water pressure? Metaphors and Analogies help connect new and unfamiliar information with ideas and experiences you already understand.</p> <p>When learning the basics of code, I started to see a variable as a box that contained \"stuff\". If I need to change the \"stuff\" in the box, I can open it up and swap out the contents. Boxes are portable so it made sense to pass variables into functions etc, like passing a box from one person to another. A constant then becomes a locked box: once something has been put inside, it is locked and cannot change.</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/how-to-remember-anything/#writing-and-repetition","title":"Writing and Repetition","text":"<p>The book that these notes are from (mentioned at the bottom if the page), consistently emphasises the importance of handwritten notes. The act of writing and re-writing/organising your notes has a notable impact on your ability to remember the content.</p> <p>With the advent of LLMs such as ChatGPT, we can get the best of both worlds: record your notes by hand, improve them, cull them etc. And once you have notes you are content with, upload a picture of the page to ChatGPT and instruct it to transcribe your hand-written notes. You can copy and paste the result into your favourite digital note app.</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/how-to-remember-anything/#combinations","title":"Combinations","text":"<p>Each of the above techniques will be worth exploring and trying out. But the real benefit will come as you learn to combine them. For example: handwriting your notes, using those notes to come up with memorable metaphors, sentences, songs etc and then finding creative ways to store those \"artifacts\" in a memory palace! To begin with, a memorable sentence may start off as a collection of chunks, but once the sentence is remembered then the whole thing becomes a single chunk and easily brought into your working memory via a memory palace.</p> <p>This post is part of a series: How We Learn</p> <ol> <li>The Two Learning Modes</li> <li>Chunk It Up</li> <li>Spaced Repetition</li> <li>How To Remember Anything</li> </ol> <p>This content was inspired by A Mind for Numbers: How to Excel at Math and Science \u2014 By Barbara Oakley, Ph.D.</p> Share this article with your friends? <p> </p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/spaced-repetition/","title":"Spaced Repetition","text":"<p>Spaced Repetition is the systematic increasing of time between recalling a piece of information. Starting regularly and eventually waiting weeks or even months as the information becomes solidified in long-term memory.</p> <p>Before we get into the \"how\" of Spaced Repetition, let's talk about what problem it solves.</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/spaced-repetition/#the-forgetting-curve","title":"The Forgetting Curve","text":"<p>When we learn a new piece of information, our brain can retain it for a small amount of time before it is forgotten. But each time we recall the information, it takes longer to forget. We have strengthened the neural pathway.</p> The Forgetting Curve","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/spaced-repetition/#spaced-repetition","title":"Spaced Repetition","text":"<p>A wonderfully logical way to combat the forgetting curve is to practice Spaced Repetition. Once you have identified an idea or concept and created a chunk of information in your mind, capture that chunk in the form of a flashcard . Read the question. Once you've thought of an answer, turn over the flashcard and check if you're right.</p> <p>Wait a few hours and then test yourself again. If you get the answer right, then wait until the following day and test yourself again. Continue this pattern of increasing the gap between each test until the knowledge is as rooted in your mind as you need it to be. But if you ever get it wrong, you have to reset and start by testing yourself the following day.</p> The Forgetting Curve affected by Spaced Repetition <p>If you get an answer wrong, use it as a chance to re-evaluate. Why did you get it wrong? Was the gap between tests was too long for a successful recall? Do you need to grapple with the content further to increase your understanding? Maybe it's a dry topic and you need to find a creative way to remember the \"boring\" topic.</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/spaced-repetition/#the-field-metaphor","title":"The Field Metaphor","text":"<p>As an example, imagine your brain as a huge overgrown field. The grass is high and the smell of fresh air is strong. You identify some new information. In our example, imagine this as a box. You need to store the info (the box) somewhere in the field. So, with box in hand, you venture out into the field to find a suitable storage spot. As you walk, some of the grass is bent by your feet and a slight parting is made in the overgrown field. A faint pathway is created. You place the box down and walk back to where you started.</p> <p>If you do nothing else, the grass will straighten, the infant pathway you made will disappear and there will be no visible evidence that you ever ventured into the field at all! The box will be lost \u2014 the information forgotten.</p> <p>But being the clever sausage that you are, you recognise this. Just as the pathway is beginning to fade, you venture out to the box \u2014 you recall the information. This act, combined with the first, strengthens the pathway. It'll now take longer before the pathway disappears.</p> <p>You repeat this process. Systematically venturing out to the box \u2014 recalling the information. Increasing the time between each trip. Until you have a dirt track. A clear pathway that is so easy to follow you can do it subconsciously. But there's a catch. If that dirt track is never walked again, it will eventually return to an overgrown field. It'll just take a long time.</p> <p>Thus, we have the forgetting curve. Each time we recall a piece of information, we are strengthening the neural pathways in our brain and lengthening the time before those pathways are overgrown and the information forgotten.</p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/spaced-repetition/#automate-with-anki","title":"Automate With Anki","text":"<p>If you are someone who loves physical flashcards then you can ignore this section.</p> <p>Anki  is a fantastic spaced repetition tool that automates a lot of the timing decisions for you. All you do is create flashcards inside the app, and the app will take care of figuring out how long to wait before asking you the question again.</p> <p>Make a habit of using Anki daily so that you benefit from the algorithm's timings. Try to hold off adding a new flashcard until you are sure the information is \"chunked\" in your mind. Little and often is the key to success. You have to be the tortoise in the race of knowledge, not the hare.</p> <p>The internet is full of Anki-related resources so I won't go over the specifics of how to use the software. A quick Google search will lead you to countless blogs and the same goes for videos on YouTube. But be warned: Anki can be insanely simple if you want it to be, or insanely complicated. Keep it simple!</p> <p>This post is part of a series: How We Learn</p> <ol> <li>The Two Learning Modes</li> <li>Chunk It Up</li> <li>Spaced Repetition</li> <li>How To Remember Anything</li> </ol> <p>This content was inspired by A Mind for Numbers: How to Excel at Math and Science \u2014 By Barbara Oakley, Ph.D.</p> Share this article with your friends? <p> </p>","tags":["education","study","memory","personal-development"]},{"location":"blog/2024/the-two-learning-modes/","title":"The Two Learning Modes","text":"<p>Your brain has two primary \"modes\" and most people don't realise just how important it is to use both of these modes together when learning something new.</p>","tags":["education","study","personal-development"]},{"location":"blog/2024/the-two-learning-modes/#focused","title":"Focused","text":"<p>This mode is pretty self-explanatory: when you are intently studying (reading, listening, watching, doing) then your brain has engaged its Focused mode. You have a narrow focus and you are detail-oriented. This kind of tunnel vision is essential for understanding new information. Your brain acknowledges and grapples with it.</p> <p>Embrace Confusion</p> <p>New information \u2014 especially in a new area of interest \u2014 will bring a natural level of confusion with it. Your brain doesn't have any pre-existing neural pathways to guide your thought process. Focused mode on its own can only get you so far. This is where Diffuse mode comes in.</p>","tags":["education","study","personal-development"]},{"location":"blog/2024/the-two-learning-modes/#diffuse","title":"Diffuse","text":"<p>Diffuse mode is the other side of the coin. The good news is that we often use it without realising, which is why doing it consciously is a game changer.</p> <p>Letting your mind wander; going out for a walk and enjoying the fresh air; taking care of that laundry pile that never goes away. These are some example of Diffuse mode activities. When your mind is completely free from thoughts of the problem at hand, then Diffuse mode can kick in.</p> <p>In this mode, ideas and information are allowed to roam the halls of your brain, building pathways between knowledge. Making connections! It's in this mode that people sometimes have that light bulb moment when understanding seems to come from nowhere. It doesn't actually come from nowhere, you just relaxed enough for your mind to make neural connections that it was unable to do in the fixated Focused mode.</p> <p>Don't expect the \"Aha!\" moment all the time. Most of the time you'll find that upon returning after a break \u2014 or even the next day \u2014 the information you grappled with is now easier to understand. </p>","tags":["education","study","personal-development"]},{"location":"blog/2024/the-two-learning-modes/#combine-the-two","title":"Combine the Two","text":"<p>When you understand the value of Focused and Diffuse modes individually, you can start to intentionally use them together. You can bounce from one to the other and speed up the digestion of new information.</p> <p>Finished a particularly gruelling article or chapter? Take a walk, go to the gym, do the dishes, listen to some instrumental music. Give yourself permission to STOP studying so that your brain can work more efficiently. There is little purpose in careering forwards into the next module before your brain has created neural connections for the module you just consumed.</p> <p>Let the foundation settle before you build on top of it.</p> <p>This post is part of a series: How We Learn</p> <ol> <li>The Two Learning Modes</li> <li>Chunk It Up</li> <li>Spaced Repetition</li> <li>How To Remember Anything</li> </ol> <p>This content was inspired by A Mind for Numbers: How to Excel at Math and Science \u2014 By Barbara Oakley, Ph.D.</p> Share this article with your friends? <p> </p>","tags":["education","study","personal-development"]},{"location":"blog/2024/to-uni-or-not-to-uni-that-is-my-question/","title":"To Uni, or Not to Uni, That Is My Question","text":"<p>I didn't go to university. It took me over a decade of professional life to discover something that I wanted to study. I'm now on my second career as a self-taught software developer.</p> <p>I have proven to myself that I can make it as a developer without formal education. But I am left wanting more. It's not about money: I don't think six years of a part-time degree would produce a monetary increase that I couldn't achieve through professional experience and grit in the meantime.</p> <p>This dilemma has played on my mind for a while now. I want knowledge. I want to identify unknown unknowns . I am attracted to the structure that education provides. But I don't want the financial burden of university \u2014 I especially don't want to pay for modules I have essentially obtained through personal study and experience.</p> <p>An idea began to grow: could I do this on my own? </p> <p>A quick search on eBay confirmed I could obtain second-hand materials for Open University modules and other university-level textbooks are available for purchase online too. Large Language Models such as ChatGPT are potentially good-enough for my needs as a tutor/professor.</p> <p>Are you saying we don't need professors anymore?</p> <p>Just to be clear, AI cannot currently replace the expertise and wisdom of an experienced university professor. Perhaps one day. But not today. AI is, however, at a point where it can provide additional insight through investigative conversations, and confirm understanding through targeted Q&amp;A sessions.</p> <p>I created my own GPT . I instructed it to behave like a CS professor and guide me to answers rather than regurgitate facts. I can even give it photos of pages in books and it will generate questions to test my comprehension! As I learn more about configuring GPTs, I will be able to improve its effectiveness.</p> <p>Next, I just needed a curriculum. I used Open University as my baseline to linearise topics. I compared their various Computing &amp; IT pathways and chose the most relevant modules for my career plans. Most of these modules have reading material available second-hand. I then used resources such as Teach Yourself CS  to identify additional materials to study. I spent some time deciding how deep I would go into each topic and disregarded a few recommendations that I felt would be overkill for my needs. I created a three-stage curriculum.</p> <p>Then I began to diverge from the typical university pathway. I began to include books considered by most to be must-reads for anyone working with code. And I fit them around the structure I had already defined.</p> <p></p> <p>But there are so many non-technical skills that compliment a career in software: leadership, finance and business to name a few. So I created a fourth stage to capture these additional skills. This stage is unstructured and designed to be studied as and when I desire. If I need a break from data structures, I'll study business or leadership. Why not?</p> <p>I published the whole curriculum on my website and called it The Self-Taught Degree. Each \"module\" has a tick box next to it so I can track my progress. I imagine it'll take me years! </p> <p>Knowledge for its own sake is futile. It needs a higher purpose. Yes, it will make me a more competent developer, but there's also an opportunity here. Considering my \"Self-Taught Degree\" will lack the status of a formal qualification, I need to prove my competence another way.</p> <p>So as I progress through each module, I shall do what any good student does: I'll take notes, converse with \"Professor GPT\", summarise content and rewrite it in my own words to confirm understanding. And then I will publish it all online. My website has been designed to become a hub of knowledge as I go through this process. Being a resource for both myself and others on their own self-taught journey.</p> <p>What a world we live in. Access to education has been a barrier for the individual progress of humans since time began. Those days are long gone. Now it's a matter of grit.</p> <p>Watch this space.</p> Share this article with your friends? <p> </p>","tags":["education","university","degree","study","personal-development"]},{"location":"blog/2024/will-artificial-intelligence-inherit-my-software-dev-job/","title":"Will Artificial Intelligence Inherit My Software Dev Job?","text":"<p>During the summer of 2018, as is common in the military, I was assigned a new role that required rapid skill development to be done by last Friday. As is not so common in the military, that skill was coding; and I was instantly hooked </p> <p>Work wasn't \"work\" anymore: it was adventure; it was exploration; it was discovery. I fondly remember the penny-drop moment when I wrote some code that looped and printed a statement ten-thousand times in an instant! The potential was exhilarating.</p>","tags":["ai","disruption","innovation","future","career"]},{"location":"blog/2024/will-artificial-intelligence-inherit-my-software-dev-job/#the-advent-of-ai-in-coding","title":"The Advent of AI in Coding","text":"<p>There were two significant moments when I realised change was on the doorstep:</p> <ol> <li> <p>A demo in 2022 of an early version of Github Copilot. You typed what you wanted the code to do and then watched as it magically generated before your eyes. I remember thinking that we had automated away the fun part of coding. Ironically, I have grown to love Github Copilot and often use it as an autocomplete for my thought process  But it does still fall over with new features of languages/frameworks \u2014 AI is only as up-to-date as its training data.</p> </li> <li> <p>The first time I held a technical conversation with ChatGPT and the weight of what that meant. It helped me understand a new concept I was grappling with. I was able to validate my comprehension (or lack thereof) by repeating the concept back in my own words and having ChatGPT confirm or correct my understanding.</p> </li> </ol>","tags":["ai","disruption","innovation","future","career"]},{"location":"blog/2024/will-artificial-intelligence-inherit-my-software-dev-job/#contemplating-the-near-future","title":"Contemplating the \"Near\" Future","text":"<p>This all leads to the question on my mind: if Artificial Intelligence can be so good at technical creation in this early stage, will it eventually inherit my job of actually writing code?</p> <p>I don't actually know the answer to this. I'm not sure anyone does...yet. If the past has taught us anything it's that the future is hard to predict. But I think it's important to contemplate potential outcomes.</p> <p>In the next few years, Artificial Intelligence could take on a role comparable to an aircraft's autopilot system: it'll do most of the heavy lifting but won't be trusted to do it unconditionally. Pilots in aircraft often do very little actual flying. They usually control take-off and landing. Then for the rest of the flight they monitor, make adjustments if required and \u2014 most importantly \u2014 are fully trained to take control in case of emergencies.</p> <p>A day in my future dev-life might look like this:</p> <ol> <li>Tell AI to generate feature X to solve problem Y</li> <li>Look over generated code \u2014 and its related unit tests, of course \u2014 to verify it works as intended. If not, adjust and repeat step 1  potential infinite loop for a stubborn mind </li> <li>If all is well, merge code into codebase</li> <li>If all is not well, oil programming hinges and put fingers to keyboard like I did in the good old days. Maybe ask ChatGPT to help...ahem</li> </ol> <p>The human will have moved from a person who creates, to a person who guides: mastering the art of leveraging AI to generate vast quantities of code that integrates into the existing codebase. Teams could end up moving at a pace we can only dream of currently.</p> <p></p>","tags":["ai","disruption","innovation","future","career"]},{"location":"blog/2024/will-artificial-intelligence-inherit-my-software-dev-job/#more-than-just-a-coder","title":"More Than Just a Coder","text":"<p>However, it\u2019s so easy to get caught up in what AI can do that I sometimes can\u2019t see the forest for the trees. The best coders write very little code. Some of the most important skills a developer can have are uniquely human.</p> <p>For a lot of companies, the developers have to engage with end-users, elicit their problems, empathise with their situations and discover creative, bespoke solutions to their needs. There are times when customers don\u2019t fully know what they need, and it requires discernment to separate the wheat from the chaff.</p> <p>Developers communicate complex technical concepts and their business value to non-technical stakeholders; this may serve as the critical determining factor for the allocation of funds.</p> <p>Consider solutions and implementations that are the result of two humans chatting casually about what they\u2019re doing \u2014 the old \u201cwatercooler effect\u201d. These situations are founded on the coming together of two different beings, often with drastically different outlooks on life. My music-teacher wife often provides a perspective on a problem that I hadn\u2019t even considered.</p> <p>What about leadership? Mentoring junior developers and onboarding new members of a team are obvious examples of leadership. But the developer who consistently includes general refactorings into their workload understands the value of quiet leadership \u2014 the kind that might get overlooked. They do it anyway because technical debt can strangle a project.</p> <p>The role of a developer is rooted in technical skills, but it's the soft skills that bolster a team and keep business goals and technology aligned. AI is currently great at the former, but not so much the latter.</p>","tags":["ai","disruption","innovation","future","career"]},{"location":"blog/2024/will-artificial-intelligence-inherit-my-software-dev-job/#ethical-considerations-and-innovation","title":"Ethical Considerations and Innovation","text":"<p>But what about further into the future? When AI can be creative intentionally, instead of on the back of hallucinations . When it can generate production-ready code consistently and communicate clearly to stakeholders. Could devs be out of a job? </p> <p>These game developers  are already pushing the limits of what AI can do. And my gut tells me that some companies will push it to the extreme. If there is a buck to be made someone will want to be the first to prove it.  Let's look at some possible consequences if this happened across the industry.</p> <p>Programming languages have \"Core Devs\": teams of people who maintain, update and improve the programming languages themselves. Their most important job is implementing security patches. They also fix bugs, improve usability etc. If AI inherits developer jobs, maybe it will inherit the job of a core dev too. Being responsible for the very code that it generates. Can AI be held responsible? </p> <p>Maybe AI will do away with the need for programming languages and default to writing machine code . Arguably, all languages are designed to be human-readable and add overhead as a result. If a human doesn't need to read it, then why bother? You'll get more efficient code.</p> <p>I hope this never happens \u2014 no matter the autonomy given to a system, we should always be able to verify what it's doing and why.</p> <p>How will innovation fare? Innovation is usually driven by determined people who are convinced they know how and why something needs to change. They often face large amounts of push back until the world either accepts the inevitable or gets onboard with the idea. Could AI innovations be as impactful as the lightbulb, the telephone or the internet?</p> <p>Maybe human innovation will flourish! There might be a world ahead of us where AI takes on so much of the mundane that humans are freed from the repetitive and empowered to innovate full-time. My job as a dev would be one of AI-empowered innovation.</p> <p>The future is a mystery. Questions abound more than answers. This may always be the case. How AI will change my role as a software developer is up for debate. I hope the changes feel like improvements to those it affects.</p> <p>Either way, I'm excited to see what happens.</p> Share this article with your friends? <p> </p>","tags":["ai","disruption","innovation","future","career"]},{"location":"blog/2025/binary-made-easy/","title":"Binary Made Easy","text":"<p>Binary numbers might seem mystifying at first glance, but this article breaks it down into familiar logic you use every day. By explaining the parallels between counting in tens and counting in twos, you'll be guided to understand how ones and zeros can represent anything!</p> <p>Joke Alert!</p> <p>\"There are 10 types of people in this world, those who understand binary and those who don't.\" \u2013 (Source Unknown)</p> <p>If you don't get this joke now, you will by the end of this article.</p> <p>Many people find binary intimidating, but it's simpler than you think. If you can count, then you can read binary. Today, we will break down this problem by reviewing how we understand numbers and then applying that same logic to binary.</p>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/binary-made-easy/#decimal-system","title":"Decimal System","text":"<p>In everyday life, when we count we typically use the decimal numbering system. In this system, a single digit is often referred to as a unit:</p> <ul> <li>0 = no units</li> <li>1 = one unit</li> <li>2 = two units, etc. </li> </ul> <p>When we reach ten units, we reset our units to zero and add another digit to the left and then continue incrementing the unit. This new digit to the left represents a multiple of 10, creating the \"tens\" column. Each column added to the left is another multiple of 10: so the pattern becomes units, tens, hundreds, thousands etc. The table below should be familiar:</p> Thousands Hundreds Tens Units Name Number 0 0 0 1 One 1 0 0 1 0 Ten 10 0 1 0 0 One Hundred 100 1 0 0 0 One Thousand 1,000 2 3 4 5 Two Thousand Three Hundred Forty-Five 2,345","tags":["computer-science","fundamentals"]},{"location":"blog/2025/binary-made-easy/#recognising-binary","title":"Recognising Binary","text":"<p>Decimal numbers can be described as \"a sequence of digits ranging from 0 to 9\". In the very same fashion, we can describe binary numbers as \"a sequence of digits ranging from 0 to 1\". The following examples are all binary numbers:</p> <ul> <li>11001100</li> <li>0001</li> <li>0</li> <li>1111</li> </ul>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/binary-made-easy/#binary-system","title":"Binary System","text":"<p>The binary numbering system is a cousin to the decimal numbering system. It is used to represent the same numbers, they just look different.</p> <p>We apply the same process to binary as we do with decimal but use only two digits (0 and 1) and each column is a multiple of 2 instead of 10. This means the pattern for our columns becomes units, twos, fours, eights, sixteens etc.</p> <p>Take some time to look over this table. Pay close attention to the column names and compare them to the decimal table above. Can you see a pattern?</p> Eight Four Two Unit Name Binary Number 0 0 0 1 One <code>0001</code> 0 0 1 0 Two <code>0010</code> 0 0 1 1 Three <code>0011</code> 0 1 0 0 Four <code>0100</code> 0 1 0 1 Five <code>0101</code>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/binary-made-easy/#converting-binary-to-decimal","title":"Converting Binary to Decimal","text":"<p>In my experience, manually converting a few binary numbers into decimal helps to solidify the concept. Let's use the binary number <code>101101</code> as an example.</p> <p>First, draw the columns around the binary number (remember to use multiples of 2). I will use \"one\" for clarity instead of \"unit\".</p> Thirty-Two Sixteen Eight Four Two One 1 0 1 1 0 1 <p>Next, convert each digit based on its position.</p> Thirty-Two Sixteen Eight Four Two One 32 0 8 4 0 1 <p>Lastly, add them up:</p> <p>\\(32 + 0 + 8 + 4 + 0 + 1 = 45\\)</p> <p>Which means <code>101101</code> equals 45. Much easier once you know how it works, right?</p> <p>This post is part of a series: Computer Science Intro</p> <ol> <li>Binary Made Easy</li> <li>How Computers Speak in Binary</li> </ol> Share this article with your friends? <p> </p>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/discover-how-computers-speak-in-binary/","title":"Discover How Computers Speak in Binary","text":"<p>Ever wondered how your computer speaks its own language? This article guides you through the basics of bits and bytes, uncovering the mechanisms computers use to store and convey data. We'll see how combinations of bits can represent anything from a single number to a video file and I'll even show you how to understand your internet speed!</p>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/discover-how-computers-speak-in-binary/#bits-and-bytes","title":"Bits and Bytes","text":"","tags":["computer-science","fundamentals"]},{"location":"blog/2025/discover-how-computers-speak-in-binary/#the-smallest-bit","title":"The Smallest Bit","text":"<p>A 'bit' is the smallest unit of data that a computer can store. It represents a binary state. The easiest way to understand this is that a bit can be either a <code>1</code> or a <code>0</code>.</p> <p>But a single bit on its own can only represent two data points: <code>1</code> or <code>0</code>. In order for a computer to make use of data, it needs more capacity. The more bits we combine together, the more unique values we can use and therefore the greater our capacity to represent data. The mathematical rule is n-bits has 2<sup>n</sup> unique values:</p> <p>1-bit makes 2 unique values: <code>0</code> &amp; <code>1</code></p> <p>2-bit makes 4 unique values: <code>00</code>, <code>01</code>, <code>10</code>, <code>11</code></p> <p>4-bit makes 16 unique values ranging from <code>0000</code> to <code>1111</code></p> <p>6-bit makes 64 unique values ranging from <code>000000</code> to <code>111111</code></p> <p>8-bit makes 256 unique values ranging from <code>00000000</code> to <code>11111111</code></p>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/discover-how-computers-speak-in-binary/#bytes-of-information","title":"Bytes of Information","text":"<p>In computing, we call an 8-bit pattern a 'byte'. As shown above, this means a single byte can store any number between 0 and 255 (256 unique values). If we need to store numbers higher than 255 then we can use multiple bytes. And as we start to use multiple bytes to represent a single data point, the range of numbers we can represent becomes staggering. As an example a 4-byte number (32 bits) has 4,294,967,296 unique values (2<sup>32</sup>)!</p> <p>\"But not all data we store is numbers\", I hear you cry! \"Some of us want to store text and emojis and stuff\" </p> <p>That is a great point. Thanks for speaking up.</p> <p>To solve this problem we create agreements. For example, when a computer is storing text it follows a standard that says the binary number <code>1100001</code> represents the letter <code>a</code>, the binary number <code>1100010</code> represents the letter <code>b</code> and so forth. Once such agreement is ASCII. Different types of data (audio, video etc) have their own agreements so that a computer can understand what the binary being stored represents.</p>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/discover-how-computers-speak-in-binary/#measuring-data-storage","title":"Measuring Data Storage","text":"<p>A single byte is an extremely small unit of storage. You could use it to store a small number or a single character. So it quickly becomes necessary to use a naming standard for large numbers of bytes.</p> <p>You've likely heard of the term 'kilobyte' before. The prefix 'kilo' is traditionally used to represent 10<sup>3</sup> (10 x 10 x 10 or 1000) as in 'kilometre' and 'kilogram'. For computers, this means a kilobyte should be 1000 bytes\u2026actually it is roughly 1000 bytes.</p> <p>What do you mean 'roughly'?</p> <p>This bit can get confusing (pun intended ) and does involve a some maths. Because computers are binary beasts, they work best with numbers that are a power of 2. As a result we bend the naming rules a bit. The nearest number to a thousand that is also a power of 2 is 2<sup>10</sup> which is 1,024. Which is why 1 kilobyte = 1,024 bytes.</p>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/discover-how-computers-speak-in-binary/#names-for-storage","title":"Names for Storage","text":"<p>kilobyte = a thousand = <code>KB</code> = 2<sup>10</sup> = <code>1,024 bytes</code></p> <p>megabyte = a million = <code>MB</code> = 2<sup>20</sup> = <code>1,024 KB</code></p> <p>gigabyte = a billion = <code>GB</code> = 2<sup>30</sup> = <code>1,024 MB</code></p> <p>terabyte = a trillion = <code>TB</code> = 2<sup>40</sup> = <code>1,024 GB</code></p> <p>petabyte = a quadrillion = <code>PB</code> = 2<sup>50</sup> = <code>1,024 TB</code></p>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/discover-how-computers-speak-in-binary/#measuring-data-transmission","title":"Measuring Data Transmission","text":"<p>Data transmission is the transfer of data from one device to another. That could be from your computer to an external hard drive. It also includes browsing the internet and watching YouTube, as your device has to download the content before you can consume it. Have you ever renewed your internet contract and wondered what the advertised speeds actually meant? This is where we shine a light on that.</p> <p>Measuring the speed of data transmission differs from measuring data storage. Data is transferred one bit at a time, albeit at an extremely fast rate! This makes measurement simple because we track the number of bits per second (bps) transferred.</p> <p>We can use the same prefixes as we do for measuring data storage (kilo, Mega, Giga, etc), with the key difference being that we can go back to using powers of 10.</p>","tags":["computer-science","fundamentals"]},{"location":"blog/2025/discover-how-computers-speak-in-binary/#names-for-transmission-speeds","title":"Names for Transmission Speeds","text":"<p>kilobits per second = a thousand = <code>kbps</code> = 10<sup>3</sup> = <code>1,000 bps</code></p> <p>megabits per second = a million = <code>Mbps</code> = 10<sup>6</sup> = <code>1,000 kbps</code></p> <p>gigabits per second = a billion = <code>Gbps</code> = 10<sup>9</sup> = <code>1,000 Mbps</code></p> <p>terabits per second = a trillion = <code>Tbps</code> = 10<sup>12</sup> = <code>1,000 Gbps</code></p> <p>What's up with the little k in kbps?</p> <p>Note the rather confusing inconsistency with capitalisation: the lowercase\u00a0'k'\u00a0represents 1,000 in decimal measurements (used to measure data transmission), unlike data storage which often uses a capital 'K' to denote binary measurement  We are not so consistent with mega, giga etc.</p> <p>This post is part of a series: Computer Science Intro</p> <ol> <li>Binary Made Easy</li> <li>How Computers Speak in Binary</li> </ol> Share this article with your friends? <p> </p>","tags":["computer-science","fundamentals"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/computer-science-fundamentals/","title":"Computer Science Fundamentals","text":""},{"location":"blog/category/learning/","title":"Learning","text":""},{"location":"blog/category/contemplations/","title":"Contemplations","text":""}]}